---
layout: default
---

<p style='text-align: justify;'>
<!-- Backdoor attacks pose a significant threat to Deep Neural Language Models by poisoning training data with desired malicious patterns. Individuals and organizations with limited resources frequently rely on obtaining pre-trained checkpoints from open-source platforms. The lack of transparency regarding the training processes increases the risk of backdoor insertion. Additionally, some emerging resource sharing or collaborative training schemes, like LoRA module sharing, federated learning, and version control based model training approaches (Git-Theta), could also be susceptible to backdoor attacks. Developing effective measures to mitigate such a malicious threat in large-scale models is essential. A fundamental initial step is to determine whether a model has been backdoored. </p> -->

<br>
<!-- Rather than relying on knowledge of poisoned data or feasible trigger sets to identify backdoored models, our goal is to identify such models in more realistic scenarios where only a collection of models is provided. -->

<br>
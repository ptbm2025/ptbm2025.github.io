---
layout: default
---

The rapid adoption of open-source Large Language Models (LLMs) has transformed natural language processing, making powerful models accessible to a broader community. But with this progress comes a growing risk: models sourced from public hubs may be compromised with hidden backdoors, inserted through fine-tuning or malicious editing.

The **Post-Training Backdoor Mitigation Challenge for Large Language Models (PTBM-LLM)** invites participants to tackle this emerging threat. Instead of relying on clean data or trusted references, this competition focuses on defending compromised models under practical, post-training constraints — the kind users often face in real-world deployment.

Participants will work across three high-impact tracks — **generative**, **classification** and **multilingual** — and explore lightweight, task-independent strategies to neutralize hidden backdoors. By offering a realistic evaluation setup and shared model checkpoints, PTBM-LLM aims to inspire new defenses that are not just effective, but also accessible.

Together, let’s build a more secure and trustworthy model-sharing ecosystem.